{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d12331f-97c3-42d8-ac9d-f22d012701a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from zamboni.models.simplenn import SimpleNN, EmbeddingNN\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd38d14-e320-413c-81b3-130ea35942a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset class to handle parquet data\n",
    "class ZamboniDataset(Dataset):\n",
    "    def __init__(self, data, target_column, cat_features=None):\n",
    "        self.feature_data = data.drop(columns=[target_column])\n",
    "        self.features = self.feature_data.values\n",
    "        self.cont_features = self.feature_data.drop(columns=cat_features).values\n",
    "        self.cat_features = data[cat_features].values\n",
    "        self.labels = data[target_column].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x_cont = torch.tensor(self.cont_features[idx], dtype=torch.float32)\n",
    "        x_cat = torch.tensor(self.cat_features[idx], dtype=torch.long)\n",
    "        y = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        if self.cat_features is not None:\n",
    "            return x_cont, x_cat, y\n",
    "        else:\n",
    "            return x_cont, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6d124b-bb96-41aa-9443-d38c3cb06e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load parquet file into a Pandas DataFrame\n",
    "data = pd.read_parquet('~/Code/zamboni/data/games.parquet')\n",
    "data = data[data['homeTeamID'] != -1]\n",
    "data = data[data['awayTeamID'] != -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad45496a-7151-4f8f-923d-4e4e5c3fa0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98a8fc6-0f09-40bf-8cf3-d8f24a36d107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define target column name\n",
    "all_columns = data.columns.tolist()\n",
    "print(all_columns)\n",
    "remove_columns = ['prevMatchupOutcome', 'prevMatchupInOT', 'hasPreviousMatchup']\n",
    "for column in remove_columns:\n",
    "    all_columns.remove(column)\n",
    "print(all_columns)\n",
    "target_column = 'outcome'\n",
    "categorical_columns = ['homeTeamID', 'awayTeamID']\n",
    "noscale_columns = [target_column] + categorical_columns\n",
    "#scale_columns = all_columns - noscale_columns\n",
    "data = data[all_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a523f1f9-ddc7-4a99-8788-f9664b51d0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test sets\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a74bf34-49db-4c82-91b6-bba580fe6144",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_teams = int(train_data['homeTeamID'].nunique()) + 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12eb6b9d-7326-4efe-8090-aa8f3a50ff1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "train_features = scaler.fit_transform(train_data.drop(columns=noscale_columns))\n",
    "test_features = scaler.transform(test_data.drop(columns=noscale_columns))\n",
    "#train_features = train_data.drop(columns=noscale_columns)\n",
    "#test_features = test_data.drop(columns=noscale_columns)\n",
    "\n",
    "# Recreate the train and test DataFrames with scaled features\n",
    "train_data_scaled = pd.DataFrame(train_features)\n",
    "test_data_scaled = pd.DataFrame(test_features)\n",
    "for column in noscale_columns:\n",
    "    train_data_scaled[column] = train_data[column].values\n",
    "    test_data_scaled[column] = test_data[column].values\n",
    "\n",
    "# Create Dataset objects\n",
    "train_dataset = ZamboniDataset(train_data_scaled, target_column, categorical_columns) \n",
    "print(len(train_dataset))\n",
    "test_dataset = ZamboniDataset(test_data_scaled, target_column, categorical_columns)\n",
    "print(len(test_dataset))\n",
    "\n",
    "# DataLoader for batching\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Define the model, loss function, and optimizer\n",
    "input_size = train_data.shape[1] - 1\n",
    "#input_size = 9\n",
    "print('input_size: ',input_size)\n",
    "hidden_size = 128\n",
    "num_classes = len(data[target_column].unique())\n",
    "num_embed_features = 2 # Embedding each team\n",
    "num_embed_categories = num_teams\n",
    "embed_dim = 10\n",
    "#model = SimpleNN(input_size, hidden_size, num_classes)\n",
    "model = EmbeddingNN(input_size, hidden_size, num_classes, num_embed_features, num_embed_categories, embed_dim)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a05231-e446-4c60-8b4e-caff04eac073",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train_loss = []\n",
    "test_loss = []\n",
    "\n",
    "# Initial loss values\n",
    "running_loss = 0\n",
    "for inputs, cat_inputs, labels in train_loader:\n",
    "    outputs = model(inputs, cat_inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "    running_loss += loss.item()\n",
    "train_loss += [running_loss]\n",
    "running_loss = 0\n",
    "for inputs, cat_inputs, labels in test_loader:\n",
    "    outputs = model(inputs, cat_inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "    running_loss += loss.item()\n",
    "test_loss += [running_loss]\n",
    "\n",
    "# Training loop\n",
    "epochs = 50\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, cat_inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs, cat_inputs)\n",
    "        #outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    train_loss += [running_loss]\n",
    "\n",
    "    model.eval()\n",
    "    running_test_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, cat_inputs, labels in test_loader:\n",
    "            outputs = model(inputs, cat_inputs)\n",
    "            #test_outputs = model(test_inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_test_loss += loss.item()\n",
    "        test_loss += [running_test_loss]\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{epochs}], Train loss: {running_loss/len(train_dataset):.4f}, Test loss: {running_test_loss/len(test_dataset):.4f}')\n",
    "\n",
    "train_loss = np.array(train_loss)\n",
    "test_loss = np.array(test_loss)\n",
    "train_loss /= len(train_dataset)\n",
    "test_loss /= len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df5c6fa-59f2-4cd4-ad9f-c26b5a82a9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(epochs+1), train_loss, label='Train')\n",
    "plt.plot(range(epochs+1), test_loss, label='Test')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5659ac1e-3fe2-4fbf-863f-f326e1c1daeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "nored_criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train_labels = np.array([])\n",
    "train_float_outputs = np.array([])\n",
    "train_outputs = []\n",
    "train_losses = []\n",
    "train_corr = 0\n",
    "test_labels = []\n",
    "test_float_outputs = []\n",
    "test_outputs = []\n",
    "test_losses = []\n",
    "test_corr = 0\n",
    "\n",
    "running_loss = 0\n",
    "for inputs, cat_inputs, labels in train_loader:\n",
    "    outputs = model(inputs, cat_inputs)\n",
    "    float_outputs = outputs.softmax(dim=1)[:,1].detach().numpy()\n",
    "    man_outputs = torch.argmax(outputs.softmax(dim=1), dim=1)\n",
    "    nored_loss = nored_criterion(outputs, labels)\n",
    "    loss = criterion(outputs, labels)\n",
    "    nored_loss = nored_criterion(outputs, labels)\n",
    "    \n",
    "    train_corr += torch.sum(man_outputs == labels)\n",
    "    running_loss += loss.item()\n",
    "    train_labels = np.concatenate([train_labels, np.array(labels)])\n",
    "    train_float_outputs = np.concatenate([train_float_outputs, np.array(float_outputs)])\n",
    "    train_outputs = np.concatenate([train_outputs, np.array(man_outputs)])\n",
    "    train_losses += nored_loss.tolist()\n",
    "    \n",
    "print(f'Train loss: {running_loss / len(train_dataset)}')\n",
    "print(f'Train accuracy: {train_corr / len(train_dataset)}')\n",
    "print()\n",
    "running_loss = 0\n",
    "for inputs, cat_inputs, labels in test_loader:\n",
    "    outputs = model(inputs, cat_inputs)\n",
    "    float_outputs = outputs.softmax(dim=1)\n",
    "    man_outputs = torch.argmax(outputs.softmax(dim=1), dim=1)\n",
    "    nored_loss = nored_criterion(outputs, labels)\n",
    "    loss = criterion(outputs, labels)\n",
    "    test_corr += torch.sum(man_outputs == labels)\n",
    "    running_loss += loss.item()\n",
    "    test_labels += labels.tolist()\n",
    "    train_float_outputs += float_outputs.tolist()\n",
    "    test_outputs += man_outputs.tolist()\n",
    "    test_losses += nored_loss.tolist()\n",
    "print(f'Test loss: {running_loss / len(test_dataset)}')\n",
    "print(f'Test accuracy: {test_corr / len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d582b4d-167d-4343-9bf8-46b43a6db31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(train_labels, alpha=0.5, label='True')\n",
    "plt.hist(train_outputs, alpha=0.5, label='Predicted')\n",
    "plt.title('Train Labels')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "one_mask = train_labels == 1\n",
    "zero_mask = train_labels == 0\n",
    "one_outputs = train_outputs[one_mask]\n",
    "zero_outputs = train_outputs[zero_mask]\n",
    "plt.hist(one_outputs, alpha=0.5, label='True 1')\n",
    "plt.hist(zero_outputs, alpha=0.5, label='True 0')\n",
    "#plt.hist(train_outputs, alpha=0.5, label='Predicted')\n",
    "plt.title('Train Prediction')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.hist(test_labels, alpha=0.5, label='True')\n",
    "plt.hist(test_outputs, alpha=0.5, label='Predicted')\n",
    "plt.title('Test Labels')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.hist(train_losses, alpha=0.5, label='Train', density=True, bins=10, range=[0,3])\n",
    "plt.hist(test_losses, alpha=0.5, label='Test', density=True, bins=10, range=[0,3])\n",
    "plt.title('Final Losses (Normalized)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#plt.savefig('tester.png')\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cecd32c-82b7-416b-954e-bdb96b874f96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
